\documentclass[12pt, titlepage]{article}
\usepackage{amsmath, mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{xr}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{xfrac}
\usepackage{tabularx}
\usepackage{float}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{pdflscape}
\usepackage{afterpage}

\usepackage[round]{natbib}

%\usepackage{refcheck}

\hypersetup{
    bookmarks=true,         % show bookmarks bar?
      colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=red,          % color of internal links (change box color with linkbordercolor)
    citecolor=green,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}


% For easy change of table widths
\newcommand{\colZwidth}{1.0\textwidth}
\newcommand{\colAwidth}{0.13\textwidth}
\newcommand{\colBwidth}{0.82\textwidth}
\newcommand{\colCwidth}{0.1\textwidth}
\newcommand{\colDwidth}{0.05\textwidth}
\newcommand{\colEwidth}{0.8\textwidth}
\newcommand{\colFwidth}{0.17\textwidth}
\newcommand{\colGwidth}{0.5\textwidth}
\newcommand{\colHwidth}{0.28\textwidth}

% Used so that cross-references have a meaningful prefix
\newcounter{defnum} %Definition Number
\newcommand{\dthedefnum}{GD\thedefnum}
\newcommand{\dref}[1]{GD\ref{#1}}
\newcounter{datadefnum} %Datadefinition Number
\newcommand{\ddthedatadefnum}{DD\thedatadefnum}
\newcommand{\ddref}[1]{DD\ref{#1}}
\newcounter{theorynum} %Theory Number
\newcommand{\tthetheorynum}{T\thetheorynum}
\newcommand{\tref}[1]{T\ref{#1}}
\newcounter{tablenum} %Table Number
\newcommand{\tbthetablenum}{T\thetablenum}
\newcommand{\tbref}[1]{TB\ref{#1}}
\newcounter{assumpnum} %Assumption Number
\newcommand{\atheassumpnum}{P\theassumpnum}
\newcommand{\aref}[1]{A\ref{#1}}
\newcounter{goalnum} %Goal Number
\newcommand{\gthegoalnum}{P\thegoalnum}
\newcommand{\gsref}[1]{GS\ref{#1}}
\newcounter{instnum} %Instance Number
\newcommand{\itheinstnum}{IM\theinstnum}
\newcommand{\iref}[1]{IM\ref{#1}}
\newcounter{reqnum} %Requirement Number
\newcommand{\rthereqnum}{P\thereqnum}
\newcommand{\rref}[1]{R\ref{#1}}
\newcounter{lcnum} %Likely change number
\newcommand{\lthelcnum}{LC\thelcnum}
\newcommand{\lcref}[1]{LC\ref{#1}}

\usepackage{fullpage}

\begin{document}

\title{Classification of Data Obfuscated By Blurring and Encryption} 
\author{Peter Michalski}
\date{\today}

\maketitle

\newpage

\tableofcontents
\addtocontents{toc}{\protect\thispagestyle{empty}}

~\newpage
\pagenumbering{gobble}
\pagenumbering{arabic}

\section{Abstract}

Preview of the paper

One paragraph (150 to 250 words)

Summarize the objective, methods, 
results, and impact

Avoid using abbreviations or citations 
in the abstract

~\newpage
\section{Introduction}

Social networking sites have leveraged data storage facilities of third parties in their business model. These third party data facilities provide the basic service of storing encrypted user data for the social service provider but could also provide advanced services such as classification of the data. 
The classified data can be used to make friend or hobby suggestions for the social media user, or could alternatively be used by the social media company or third party for the development of privacy protected meta-data for resale. 

Currently studies have focused on the classification of data that has been obfuscated by blurring or encryption techniques. Ahmed et al have focused on using VGGNet and auto-encoder networks to classify images that have been blurred and encrypted. 

This study will attempt to use similar VGGNet and auto-encoder networks from Ahmed et al to classify images that have been obfuscated by both blurring and encryption. The objective of this, using the social media model, is to enable third party data storage facilities to make friend or hobby suggestions or gather meta-data using noisy data. As users are unlikley to upload noisy single images to social media sites, our model will focus on noisy data that is uploaded in the form of video frames. A poor quality video camera, electrical interference, and unstable recording could result in a non negligible amount of noisy video frames uploaded into the third party server. This study will attempt to measure the effectiveness of classification of objects within such noisy frames for the purpose of generating useful user data. 

Motivation: (mix with above)
In this scenario, noisy photos and/or videos are encrypted and uploaded to a cloud server which then classifies the images for product and/or friend recommendations. The videos are encrypted due to security risks with the cloud server, including due to a scenario where the cloud server is a third party service tasked with classification but without the authority to view unencrypted content. This is a similar application to the work of Ahmed et al. discussed in Section \ref{Related_Work}; however, the subject would be uploading noisy photos or video frames due to a poor quality camera or unstable recording. In such a scenario the subject would likely be uploading all of their digital content from their hardware without manually filtering for good quality images.

Impact: 
Along with the scenario listed above, the impact of this research can be extended to any application that may attempt to classify noisy data that has been encrypted for the sake of privacy. Noisy passive security camera footage may be classified by third party data storage systems. Analog data that is securely stored can also be classified for anomalies or meta-features by third party storage systems. Overall, the impact of the study will reveal how effective the studies classification models are in extracting features from over processed data. 


The VGGNet and auto-encoder models found in Ahmed et al have not been used to classify images that have been blurred and then encrypted. We will first use these models because... their strengths are... weaknesses may be...

Upon classification using the above models, we will attempt to add... this could have the effect of...

The majo rcontributions are...ahmed et al...other paper...vggnet...eutoencoder..

The organization of the paper is...



the introduction addresses what the study will entail. 

What is the problem to be solved

Why it is important (motivation, impact)

Why your solution is novel and suitable 
to solve the problem

Why previous efforts do not work

What is the brief idea

What are the major Contributions

Organization of the paper\\

Research Questions:
\begin{enumerate}
	\item How effective is the classifier from Ahmed et al.'s paper on MNIST and CIFAR-10 data sets when the images have first been blurred using the Gaussian technique (across varying standard deviations) and then encrypted using AES-128 ECB?
	\item Can Ahmed et al's classifier be improved to classify blurred and then encrypted images using the residual learning and batch normalization techniques from Zhang et al.'s paper?
\end{enumerate}

\subsection{Problem Statement}\label{ps}
There is considerable research that has been done on the subject of obfuscated image classification using neural networks. Recent research, some of which is outlined in Section \ref{Related_Work}, focuses on the classification of images that have been obfuscated using blurring or encryption. First, this project aims to use a current classification model to classify images that have been blurred and encrypted, and then analyze the results. The second half of this project aims to explore the development of a new model by testing the combination of different methods to classify images that have been blurred and encrypted.

\subsection{Motivations}\label{motivations}
There are several applications for which this project could contribute to developing a solution:

\subsubsection{Grainy Photos Uploaded for Secure Recommendation}
In this scenario, noisy photos and/or videos are encrypted and uploaded to a cloud server which then classifies the images for product and/or friend recommendations. The videos are encrypted due to security risks with the cloud server, including due to a scenario where the cloud server is a third party service tasked with classification but without the authority to view unencrypted content. This is a similar application to the work of Ahmed et al. discussed in Section \ref{Related_Work}; however, the subject would be uploading noisy photos or video frames due to a poor quality camera or unstable recording. In such a scenario the subject would likely be uploading all of their digital content from their hardware without manually filtering for good quality images.

\subsubsection{Grainy Security Footage Requiring Secure Categorization}
In this scenario, noisy security photos and/or video frames due to interference or poor hardware are encrypted and sent to storage where categorization of such encrypted passive footage is required. This scenario could occur if a third party service is tasked with classification, perhaps for the sake of some general security analysis, but does not have the authority to view unencrypted content. The scenario could similarly occur if the same party that recorded the footage is tasked with classification, once again for the sake of general security analysis, but also without the authority to view unencrypted content. The latter scenario could be a government entity that records clandestine security footage for analysis but does not have the legal authority to hold the unencrypted footage, perhaps due to privacy laws. 


\subsection{Objectives}\label{objectives}
The first objective of this study is to classify obfuscated images, using current image classification models, that have been both blurred and encrypted. This is novel as currently classification of obfuscated images deals with images that have been either blurred or encrypted. The second objective is to begin work on a novel classifier that will be specifically designed to classify images that have been blurred and then encrypted.

~\newpage
\section{Related Work}
The problem addressed in this paper is the classification of retrieved objects from obfuscated data. The works discussed below address related studies and relevant data classification approaches.

Maybe each should be a subsection?
\begin{enumerate}
	\item ``Obfuscated image classification for secure image-centric friend recommendation" by Ahmed et al.\citep{ahmed2018obfuscated}:\\
	
	Ahmed et al. focus on the classification of images that have been obfuscated using one of several techniques, including blurring or encryption. Their motivation is a scenario where a third party will need to classify secure private social media images for the purpose of friend recommendations. 
	
	\begin{figure}[h!]
		\begin{center}
			\includegraphics[width=0.8\textwidth]{ahmed_social_model}
			\caption{Social Model Architecture}
			\label{Ahmed_social_model}
		\end{center}
	\end{figure}
	
	
	They use Gaussian blurring in two dimensions and AES-128 encryption in ECB block cipher mode for such obfuscation of MNIST, CIFAR-10, and MirFlickr-25K data sets. Ahmed et al. classify these obfuscated data-sets using an auto-encoder neural network as well as a deep convolutional neural network based on Simonyan and Zisserman's Visual Geometry Group (VGGNet) \cite{simonyan2014very}. Specifically, they classify encrypted images using auto-encoder and blurred images using a deep convolutional network. 
	The results are discussed in Section \ref{Background} of this report. \\
	
	\item ``Very Deep Convolutional Networks for Large-Scale Image Recognition"
	\citep{simonyan2014very}:
	Simonyan and Zisserman evaluate networks of increased depth, 11-19 layers, and very small convolutional filters (3 x 3). They describe several network configurations for validating and classifying 224 x 224 RGB images. The networks incorporate several convolutional layers and filters, several max-pooling layers, and several fully-connected layers. The hidden layers of their networks are equipped with ReLU.
	
	\begin{figure}[h!]
		\begin{center}
			\includegraphics[width=0.8\textwidth]{ConvNet}
			\caption{ConvNet Configurations}
			\label{ConvNet}
		\end{center}
	\end{figure}
	
	The ConvNet training is carried out by optimizing the multinational logistic regression objective using mini-batch gradient descent. The paper demonstrated that an increased representation depth is beneficial for classification accuracy.
	
	\item ``Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising" by Zhang et al.\citep{zhang2017beyond}:\\
	
	REMOVE RESIDUAL LEARNING
	
	This paper focuses on a feed forward denoising convolutional neural networks - specifically for Gaussian denoising. The proposed DnCNN uses residual learning formulation to learn and batch normalization to improve speed and performance. Residual learning uses residual mapping to allow information to flow more easily between layers of a deep neural network
	Batch normalization alleviates the `internal covariate shift' by incorporating a normalization step and a scale and shift step.
	
	The contributions of this work are summarized as follows:
	
	We propose an end-to-end trainable deep CNN for Gaussian denoising. In contrast to the existing deep neural network-based methods which directly estimate the latent clean image, the network adopts the residual learning strategy to remove the latent clean image from noisy observation.
	
	We find that residual learning and batch normalization can greatly benefit the CNN learning as they can not only speed up the training but also boost the denoising performance. For Gaussian denoising with a certain noise level, DnCNN outperforms state-of-the-art methods in terms of both quantitative metrics and visual quality.
	
	Our DnCNN can be easily extended to handle general image denoising tasks. We can train a single DnCNN model for blind Gaussian denoising, and achieve better performance than the competing methods trained for a specific noise level. Moreover, it is promising to solve three general image denoising tasks, i.e., blind Gaussian denoising, SISR, and JPEG deblocking, with only a single DnCNN model.
	
	 The proposed model of the paper has a convolutional and rectified linear unit (ReLU) first layer which uses filters to generate feature maps and ReLU for nonlinearlity. The model then has several convolutional, batch normalization, and ReLU layers. Finally a convolutional layer is used to reconstruct the output. The results of this model are a higher peak signal to noise ratio (PSNR) than competing denoisers on the BSD68 data set. 
	
	Extensive experiments show that, our DnCNN trained with a certain noise level can yield better Gaussian denoising results than state-of-the-art methods 
	
	\item ``Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift''
	
	This is the main batch normalization paper?
\end{enumerate}

~\newpage
\section{Background}\label{Background}

discuss VGGNet and autoencoder, as well as batch normalization, and present results from Ahmed et al (and other paper?) and then point to the results I got from trying to emulate their models (prior to BL + EC or batch normalization)

----from the paper:


2) Batch Normalization:

Mini-batch stochastic gradient descent (SGD) has been widely used in training CNN models. Despite the simplicity and effectiveness of mini-batch SGD, its training efficiency is largely reduced by internal covariate shift [28], i.e., changes in the distributions of internal nonlinearity inputs during training. Batch normalization [28] is proposed to alleviate the internal covariate shift by incorporating a normalization step and a scale and shift step before the nonlinearity in each layer. For batch normalization, only two parameters per activation are added, and they can be updated with back-propagation. Batch normalization enjoys several merits, such as fast training, better performance, and low sensitivity to initialization. For further details on batch normalization, please refer to [28].

By far, no work has been done on studying batch normalization for CNN-based image denoising. We empirically find that, the integration of batch normalization can result in fast and stable training and better denoising performance.

----



The background discusses existing data on your topic, the problem statement is what you identify as an issue with supporting data. 

Have figures here from the related work.

The pr



Maybe problem statement at end of this?

Also view her email to me

~\newpage
\section{Methodology}

Our study will focus on classification of obfuscated data by blurring and encryption using models similar to those used by Ahmed et al. - a VGGNet and an autoencoder network.

We will be examining classification of 4 sets of data on both the VGGNet and autoencode style models found in Ahmed et al. We will then re-examine classification of these 4 sets on these 2 models after the models have been adjusted to include 


\subsection{Image Obfuscation}
\subsubsection{Blurring}\label{BlurringMethodology}
\subsubsection{Encryption}\label{EncryptionMethodology}
We will be using th MNIST dataset to conduct our classification....describe it.

The first set of data will be non-secure MNIST, the second set will be MNIST obfuscated by blurring using Gaussian..., the third set will be MNIST obfuscated by encrypted using block encrytpion, the fourth will be MNIST obfuscated blurred using Gaussian and then encrptyed using block encyprtion. 

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{MNIST_BASE}
		\caption{Non-Secure MNIST Image Samples}
		\label{MNIST_BASE}
	\end{center}
\end{figure}


Obfuscation of the data using the blurred method will be conducted...the blurred will be conducted along a range of xxxx values.  

 For the blurring, we have used the Matlab image filtering toolbox imnoise
 
 
 \begin{figure}[h!]
 	\begin{center}
 		\includegraphics[width=0.8\textwidth]{MNIST_BLUR}
 		\caption{Blurred MNIST}
 		\label{MNIST_BLURRED}
 	\end{center}
 \end{figure}

 
 Obfuscation of the data using the encryption method will be conducted using block encryption...link..how does block encrpytion work? state simialr to aes ecb 128 used by ahmed et al..
 
\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{MNIST_EN}
		\caption{Encrypted MNIST}
		\label{MNIST_ENCRYPTED}
	\end{center}
\end{figure}


Blurring and encryption will combine...along the range...

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{MNIST_BLUR_ER}
		\caption{Blurred and Encrypted MNIST}
		\label{MNIST_BLURRED_EN}
	\end{center}
\end{figure} 
 
\subsection{Original VGGNet} 

VGGNet based on .... which we will refer to as 'original VGGNet' in the remainder of this report

\subsection{Modified VGGNet} 

\subsection{Original Autoencoder} 

Autoencoder based on .... which we will refer to as 'original Autoencoder' in the remainder of this report

\subsection{Modified Autoencoder} 
 
 ~\newpage
\section{Evaluations}
\subsection{Original VGGNet}\label{EvalOrigVGG}

The classification result for the non-secure MNIST data set is found in Table \ref{table:basicVGG_MNIST}: Non-Secure MNIST Classification Results: Original VGGNet. The data set produced an accuracy of 99.31\% averaged over three tests. This accuracy is similar to the 99.503\% accuracy of Ahmed et al. for non-secure MNIST.\\ 

\begin{table}[!h]
	\begin{center}
		\begin{tabular}{| c | c |}
			\hline
			\textbf{Data-Set} & \textbf{Accuracy}\\
			\hline
			MNIST & 0.9931\\
			\hline
		\end{tabular}
		\caption{Non-Secure MNIST Classification Results: Original VGGNet}
		\label{table:basicVGG_MNIST}
	\end{center}
\end{table}

\noindent The classification result for the encrypted MNIST data set is found in Table \ref{table:basicVGG_Encryption}: Encrypted MNIST Classification Results: Original VGGNet. The data set produced an accuracy of 96.03\% averaged over three tests. This accuracy is significantly greater than the 83.93\% accuracy of Ahmed et al. for encrypted MNIST. This difference is attributed to using a novel simple block encryption algorithm for the purpose of maintaining convergence of our models when classifying images that have been both blurred and encrypted, as outlined in Section \ref{EncryptionMethodology}: Encryption.\\

\begin{table}[!h]
	\begin{center}
		\begin{tabular}{| c | c |}
			\hline
			\textbf{Data-Set} & \textbf{Accuracy}\\
			\hline
			Encrypted MNIST & 0.9603\\
			\hline
		\end{tabular}
		\caption{Encrypted MNIST Classification Results: Original VGGNet}
		\label{table:basicVGG_Encryption}
	\end{center}
\end{table}

\noindent The classification result for the blurred MNIST data sets, averaged over three tests, is found in Table \ref{table:basicVGG_Blurred}: Blurred MNIST Classification Results: Original VGGNet. The highest accuracy of the data sets was 99.04\% for the Var0\_04 data set, which has the lowest variance of Gaussian noise at 0.04, as outlined in Section \ref{BlurringMethodology}: Blurring. The lowest accuracy of the data sets was 89.60\% for the Var0\_60 data set, which has the highest variance of Gaussian noise at 0.60. A linear increase in variance is observed to cause an exponential decrease in accuracy. As a comparison, the average classification accuracy in Ahmed et al. for blurred MNIST was 95.93\%, suggesting that our blurring technique has similar effect.

\begin{table}[!h]
	\begin{center}
		\begin{tabular}{| c | c |}
			\hline
			\textbf{Data-Set} & \textbf{Accuracy}\\
			\hline
			Var0\_04 & 0.9904\\
			\hline
			Var0\_12 & 0.9883\\
			\hline
			Var0\_20 & 0.9763\\
			\hline
			Var0\_28 & 0.9656\\
			\hline
			Var0\_36 & 0.9566\\
			\hline
			Var0\_44 & 0.9441\\
			\hline
			Var0\_52 & 0.9282\\
			\hline
			Var0\_60 & 0.8960\\
			\hline
		\end{tabular}
		\caption{Blurred MNIST Classification Results: Original VGGNet}
		\label{table:basicVGG_Blurred}
	\end{center}
\end{table}

~\newpage
\noindent The classification result for the blurred and encrypted MNIST data sets, averaged over three tests, is found in Table \ref{table:basicVGG_BlurredEncrypted}: Blurred and Encrypted MNIST Classification Results: Original VGGNet. The highest accuracy of the data sets was 56.29\% for the 0\_04\_en data set, which has the lowest variance of Gaussian noise at 0.04, as outlined in Section \ref{BlurringMethodology}: Blurring. The lowest accuracy of the data sets was 47.29\% for the 0\_60\_en data set, which has the highest variance of Gaussian noise at 0.60. Similar to the blurred MNIST data sets, a linear increase in variance is observed to cause an exponential decrease in accuracy in these blurred and encrypted data sets. The rate of change of accuracy relative to variance of Gaussian noise is inherited from the blurred data sets as they are further encrypted.\\ 

\begin{table}[!h]
	\begin{center}
		\begin{tabular}{| c | c |}
			\hline
			\textbf{Data-Set} & \textbf{Accuracy}\\
			\hline
			0\_04\_en & 0.5629\\
			\hline
			0\_12\_en & 0.5794\\
			\hline
			0\_20\_en & 0.5781\\
			\hline
			0\_28\_en & 0.5599\\
			\hline
			0\_36\_en & 0.5609\\
			\hline
			0\_44\_en & 0.4866\\
			\hline
			0\_52\_en & 0.4878\\
			\hline
			0\_60\_en & 0.4729\\
			\hline
		\end{tabular}
		\caption{Blurred and Encrypted MNIST Classification Results: Original VGGNet}
		\label{table:basicVGG_BlurredEncrypted}
	\end{center}
\end{table}

~\newpage
\noindent A graphical presentation of the classification results for the blurred MNIST data sets compared to the blurred and encrypted MNIST data sets can be observed in Figure \ref{GRAPH_OriginalVGG}: Original VGGNet Accuracy Comparison. As stated previously, we can observe that both sets inherit the characteristic that a linear increase in the variance of Gaussian noise results in an exponential decrease in classification accuracy. We can also observe that further encryption has resulted in a profound decrease in classification accuracy for all blurred data sets. While the encrypted-only MNIST produced a high classification accuracy result of 96.03\%, as stated in Table \ref{table:basicVGG_Encryption}, the encryption of the blurred-only data sets with such a mild encryption algorithm has profoundly decreased their classification accuracy, as found in Table \ref{table:basicVGG_BlurredEncrypted} and observed in Figure \ref{GRAPH_OriginalVGG} below. The encryption of the fairly accurate blurred data sets has resulted in a drop of roughly 40\% in accuracy.


\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{O_VGG_BL_vs_BEN}
		\caption{Original VGGNet Accuracy Comparison}
		\label{GRAPH_OriginalVGG}
	\end{center}
\end{figure} 

~\newpage
\subsection{Modified VGGNet}\label{EvalModVGG}

The classification result for the non-secure MNIST data set is found in Table \ref{table:modVGG_MNIST}: Non-Secure MNIST Classification Results: Modified VGGNet. The data set produced an accuracy of 99.39\% averaged over three tests. The accuracy is slightly improved to the 99.31\% accuracy result of the original VGGNet neural network found in Section \ref{EvalOrigVGG}.\\ 

\begin{table}[!h]
	\begin{center}
		\begin{tabular}{| c | c |}
			\hline
			\textbf{Data-Set} & \textbf{Accuracy}\\
			\hline
			MNIST & 0.9939\\
			\hline
		\end{tabular}
		\caption{Non-Secure MNIST Classification Results: Modified VGGNet}
		\label{table:modVGG_MNIST}
	\end{center}
\end{table}

\noindent The classification result for the encrypted MNIST data set is found in Table \ref{table:modVGG_Encryption}: Encrypted MNIST Classification Results: Modified VGGNet. The data set produced an accuracy of 98.21\% averaged over three tests. The change in accuracy is non-negligible when compared to the 96.03\% accuracy of the original VGGNet found in Section \ref{EvalOrigVGG}. The addition of batch normalization to the original VGGNet has decreased inaccurate classification of encrypted MNIST images by roughly half.\\

\begin{table}[!h]
	\begin{center}
		\begin{tabular}{| c | c |}
			\hline
			\textbf{Data-Set} & \textbf{Accuracy}\\
			\hline
			Encrypted MNIST & 0.9821\\
			\hline
		\end{tabular}
		\caption{Encrypted MNIST Classification Results: Modified VGGNet}
		\label{table:modVGG_Encryption}
	\end{center}
\end{table}

\noindent The classification result for the blurred MNIST data sets, averaged over three tests, is found in Table \ref{table:modVGG_Blurred}: Blurred MNIST Classification Results: Modified VGGNet. The highest accuracy of the data sets was 99.21\% for the Var0\_04 data set, which has the lowest variance of Gaussian noise at 0.04, as outlined in Section \ref{BlurringMethodology}. The lowest accuracy of the data sets was 90.31\% for the Var0\_60 data set, which has the highest variance of Gaussian noise at 0.60. Similar to the results of the original VGGNet, as found in section \ref{EvalOrigVGG}, a linear increase in variance is observed to cause an exponential decrease in accuracy.

\begin{table}[!h]
	\begin{center}
		\begin{tabular}{| c | c |}
			\hline
			\textbf{Data-Set} & \textbf{Accuracy}\\
			\hline
			Var0\_04 & 0.9921\\
			\hline
			Var0\_12 & 0.9891\\
			\hline
			Var0\_20 & 0.9812\\
			\hline
			Var0\_28 & 0.9704\\
			\hline
			Var0\_36 & 0.9585\\
			\hline
			Var0\_44 & 0.9468\\
			\hline
			Var0\_52 & 0.9298\\
			\hline
			Var0\_60 & 0.9031\\
			\hline
		\end{tabular}
		\caption{Blurred MNIST Classification Results: Modified VGGNet}
		\label{table:modVGG_Blurred}
	\end{center}
\end{table}

~\newpage
\noindent The classification result for the blurred and encrypted MNIST data sets, averaged over three tests, is found in Table \ref{table:modVGG_BlurredEncrypted}: Blurred and Encrypted MNIST Classification Results: Modified VGGNet. The highest accuracy of the data sets was 91.40\% for the 0\_04\_en data set, which has the lowest variance of Gaussian noise at 0.04, as outlined in Section \ref{BlurringMethodology}: Blurring. The lowest accuracy of the data sets was 81.06\% for the 0\_60\_en data set, which has the highest variance of Gaussian noise at 0.60. Similar to the results of the original VGGNet found in Section \ref{EvalOrigVGG}, a linear increase in variance is observed to cause an exponential decrease in accuracy in these blurred and encrypted data sets. Section \ref{VGGAnalysis}: VGGNet Analysis addresses the observation that classification accuracy results for blurred and encrypted data sets is significantly higher in the modified VGG when compared to the original VGG.\\ 

\begin{table}[!h]
	\begin{center}
		\begin{tabular}{| c | c |}
			\hline
			\textbf{Data-Set} & \textbf{Accuracy}\\
			\hline
			0\_04\_en & 0.9140\\
			\hline
			0\_12\_en & 0.9076\\
			\hline
			0\_20\_en & 0.8942\\
			\hline
			0\_28\_en & 0.8813\\
			\hline
			0\_36\_en & 0.8688\\
			\hline
			0\_44\_en & 0.8491\\
			\hline
			0\_52\_en & 0.8323\\
			\hline
			0\_60\_en & 0.8106\\
			\hline
		\end{tabular}
		\caption{Blurred and Encrypted MNIST Classification Results: Modified VGGNet}
		\label{table:modVGG_BlurredEncrypted}
	\end{center}
\end{table} 

~\newpage
\noindent A graphical presentation of the classification results for the blurred MNIST data sets compared to the blurred and encrypted MNIST data sets can be observed in Figure \ref{GRAPH_ModifiedVGG}: Modified VGGNet Accuracy Comparison. As stated previously, we can observe that both sets inherit the characteristic that a linear increase in the variance of Gaussian noise results in an exponential decrease in classification accuracy. We can also observe that further encryption has resulted in a marginal decrease in classification accuracy for all blurred data sets. While the encrypted-only MNIST produced a high classification accuracy result of 98.21\%, as stated in Table \ref{table:modVGG_Encryption}, the encryption of the blurred-only data sets with such a mild encryption algorithm has marginally decreased their classification accuracy, as found in Table \ref{table:modVGG_BlurredEncrypted} and observed in Figure \ref{GRAPH_ModifiedVGG} below. The encryption of the blurred data sets has resulted in a drop of roughly 10\% in accuracy.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{Mod_VGG_BL_vs_BEN}
		\caption{Modified VGGNet Accuracy Comparison}
		\label{GRAPH_ModifiedVGG}
	\end{center}
\end{figure} 

~\newpage
\subsection{VGGNet Analysis}\label{VGGAnalysis}

The classification results of the original and modified VGGNet for the blurred MNIST data sets can be observed in Figure \ref{GRAPH_COMP_VGG_BL}: VGGNet Accuracy Comparison - Blurred. We observe that the addition of batch normalization has not significantly increased classification accuracy in the modified VGGNet. The exponential effect of linearly increasing the variance of Gaussian noise is seen in both neural networks.\\

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{Mod_vs_Or_VGG_BL}
		\caption{VGGNet Accuracy Comparison - Blurred}
		\label{GRAPH_COMP_VGG_BL}
	\end{center}
\end{figure}

~\newpage
\noindent The classification results of the original and modified VGGNet for the blurred and encrypted MNIST data sets can be observed in Figure \ref{GRAPH_COMP_VGG_BEN}: VGGNet Accuracy Comparison - Blurred and Encrypted. We observe that the addition of batch normalization has significantly increased classification accuracy in the modified VGGNet. The data set with the highest accuracy when using the original VGGNet, 0\_04\_en with 56.29\%, has an accuracy of 91.40\% when using the modified VGGNet. The data set with the lowest accuracy when using the original VGGNet, 0\_60\_en with 47.29\%, has an accuracy of 81.06\% when using the modified VGGNet. The stabilizing effect of batch normalization on internal covariate shift is clear from these results.\\

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{Mod_vs_Or_VGG_BEN}
		\caption{VGGNet Accuracy Comparison - Blurred and Encrypted}
		\label{GRAPH_COMP_VGG_BEN}
	\end{center}
\end{figure}

~\newpage
\subsection{Original Autoencoder}\label{EvalOrigAE} 

The classification result for the non-secure MNIST data set is found in Table \ref{table:basicAE_MNIST}: Non-Secure MNIST Classification Results: Original Autoencoder. The data set produced an accuracy of 98.64\% averaged over three tests. This accuracy is similar to the 99.503\% accuracy of Ahmed et al. for non-secure MNIST.\\ 

\begin{table}[!h]
	\begin{center}
		\begin{tabular}{| c | c |}
			\hline
			\textbf{Data-Set} & \textbf{Accuracy}\\
			\hline
			MNIST & 0.9864\\
			\hline
		\end{tabular}
		\caption{Non-Secure MNIST Classification Results: Original Autoencoder}
		\label{table:basicAE_MNIST}
	\end{center}
\end{table}

\noindent The classification result for the encrypted MNIST data set is found in Table \ref{table:basicAE_Encryption}: Encrypted MNIST Classification Results: Original Autoencoder. The data set produced an accuracy of 96.98\% averaged over three tests. Similar to the VGGNet model, this accuracy is significantly greater than the 83.93\% accuracy of Ahmed et al. for encrypted MNIST. This difference is attributed to using a novel simple block encryption algorithm for the purpose of maintaining convergence of our models when classifying images that have been both blurred and encrypted, as outlined in Section \ref{EncryptionMethodology}: Encryption.\\

\begin{table}[!h]
	\begin{center}
		\begin{tabular}{| c | c |}
			\hline
			\textbf{Data-Set} & \textbf{Accuracy}\\
			\hline
			Encrypted MNIST & 0.9698\\
			\hline
		\end{tabular}
		\caption{Encrypted MNIST Classification Results: Original Autoencoder}
		\label{table:basicAE_Encryption}
	\end{center}
\end{table}

\noindent The classification result for the blurred MNIST data sets, averaged over three tests, is found in Table \ref{table:basicAE_Blurred}: Blurred MNIST Classification Results: Original Autoencoder. The highest accuracy of the data sets was 98.94\% for the Var0\_04 data set, which has the lowest variance of Gaussian noise at 0.04, as outlined in Section \ref{BlurringMethodology}: Blurring. The lowest accuracy of the data sets was 89.77\% for the Var0\_60 data set, which has the highest variance of Gaussian noise at 0.60. A linear increase in variance is once again observed to cause an exponential decrease in accuracy. As a comparison, the average classification accuracy in Ahmed et al. for blurred MNIST was 95.93\%, suggesting that our blurring technique has similar effect.

\begin{table}[!h]
	\begin{center}
		\begin{tabular}{| c | c |}
			\hline
			\textbf{Data-Set} & \textbf{Accuracy}\\
			\hline
			Var0\_04 & 0.9894\\
			\hline
			Var0\_12 & 0.9838\\
			\hline
			Var0\_20 & 0.9739\\
			\hline
			Var0\_28 & 0.9644\\
			\hline
			Var0\_36 & 0.9461\\
			\hline
			Var0\_44 & 0.9147\\
			\hline
			Var0\_52 & 0.9193\\
			\hline
			Var0\_60 & 0.8977\\
			\hline
		\end{tabular}
		\caption{Blurred MNIST Classification Results: Original Autoencoder}
		\label{table:basicAE_Blurred}
	\end{center}
\end{table}

~\newpage
\noindent The classification result for the blurred and encrypted MNIST data sets, averaged over three tests, is found in Table \ref{table:basicAE_BlurredEncrypted}: Blurred and Encrypted MNIST Classification Results: Original Autoencoder. The highest accuracy of the data sets was 46.88\% for the 0\_04\_en data set, which has the lowest variance of Gaussian noise at 0.04, as outlined in Section \ref{BlurringMethodology}: Blurring. The lowest accuracy of the data sets, from data sets that converged on a solution, was 17.29\% for the 0\_36\_en data set, which has a variance of Gaussian noise of 0.36. Data sets with a higher variance of Gaussian noise did not converge on a solution and are marked as DNC (Did Not Converge). Similar to the blurred MNIST data sets, a linear increase in variance is observed to cause an exponential decrease in accuracy in these blurred and encrypted data sets. The rate of change of accuracy relative to variance of Gaussian noise is inherited from the blurred data sets as they are further encrypted.\\ 

\begin{table}[!h]
	\begin{center}
		\begin{tabular}{| c | c |}
			\hline
			\textbf{Data-Set} & \textbf{Accuracy}\\
			\hline
			0\_04\_en & 0.4688\\
			\hline
			0\_12\_en & 0.4365\\
			\hline
			0\_20\_en & 0.4301\\
			\hline
			0\_28\_en & 0.3193\\
			\hline
			0\_36\_en & 0.1729\\
			\hline
			0\_44\_en & DNC\\
			\hline
			0\_52\_en & DNC\\
			\hline
			0\_60\_en & DNC\\
			\hline
		\end{tabular}
		\caption{Blurred and Encrypted MNIST Classification Results: Original Autoencoder}
		\label{table:basicAE_BlurredEncrypted}
	\end{center}
\end{table}

~\newpage
\noindent A graphical presentation of the classification results for the blurred MNIST data sets compared to the blurred and encrypted MNIST data sets can be observed in Figure \ref{GRAPH_OriginalAE}: Original Autoencoder Accuracy Comparison. As stated previously, we can observe that both sets inherit the characteristic that a linear increase in the variance of Gaussian noise results in an exponential decrease in classification accuracy. This is more apparent in the blurred and encrypted data sets for the Autoencoder neural network. We can also observe that further encryption has resulted in a profound decrease in classification accuracy for all blurred data sets, with the most blurred data sets not converging on a solution. While the encrypted-only MNIST produced a high classification accuracy result of 96.98\%, as stated in Table \ref{table:basicAE_Encryption}, the encryption of the blurred-only data sets with such a mild encryption algorithm has significantly decreased their classification accuracy, as found in Table \ref{table:basicAE_BlurredEncrypted} and observed in Figure \ref{GRAPH_OriginalAE} below. The encryption of the fairly accurate blurred data sets has resulted in a drop of more than 50\% in accuracy. The three data sets with the highest variance of Gaussian noise did not converge.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{OAE_B_vs_BEN}
		\caption{Original Autoencoder Accuracy Comparison}
		\label{GRAPH_OriginalAE}
	\end{center}
\end{figure} 

\subsection{Modified Autoencoder}\label{EvalModAE}

The classification result for the non-secure MNIST data set is found in Table \ref{table:modAE_MNIST}: Non-Secure MNIST Classification Results: Modified Autoencoder. The data set produced an accuracy of 98.65\% averaged over three tests. The accuracy is similar to the 98.64\% accuracy result of the original Autoencoder neural network found in Section \ref{EvalOrigAE}.\\ 

\begin{table}[!h]
	\begin{center}
		\begin{tabular}{| c | c |}
			\hline
			\textbf{Data-Set} & \textbf{Accuracy}\\
			\hline
			MNIST & 0.9865\\
			\hline
		\end{tabular}
		\caption{Non-Secure MNIST Classification Results: Modified Autoencoder}
		\label{table:modAE_MNIST}
	\end{center}
\end{table}

\noindent The classification result for the encrypted MNIST data set is found in Table \ref{table:modAE_Encryption}: Encrypted MNIST Classification Results: Modified Autoencoder. The data set produced an accuracy of 97.27\% averaged over three tests. The change in accuracy is marginal when compared to the 96.98\% accuracy of the original Autoencoder found in Section \ref{EvalOrigAE}. The addition of batch normalization to the original Autoencoder has only slightly decreased inaccurate classification of encrypted MNIST images.\\

\begin{table}[!h]
	\begin{center}
		\begin{tabular}{| c | c |}
			\hline
			\textbf{Data-Set} & \textbf{Accuracy}\\
			\hline
			Encrypted MNIST & 0.9727\\
			\hline
		\end{tabular}
		\caption{Encrypted MNIST Classification Results: Modified Autoencoder}
		\label{table:modAE_Encryption}
	\end{center}
\end{table}

\noindent The classification result for the blurred MNIST data sets, averaged over three tests, is found in Table \ref{table:modAE_Blurred}: Blurred MNIST Classification Results: Modified Autoencoder. The highest accuracy of the data sets was 98.67\% for the Var0\_04 data set, which has the lowest variance of Gaussian noise at 0.04, as outlined in Section \ref{BlurringMethodology}. The lowest accuracy of the data sets was 90.93\% for the Var0\_60 data set, which has the highest variance of Gaussian noise at 0.60. Similar to the results of the original Autoencoder, as found in section \ref{EvalOrigAE}, a linear increase in variance is observed to cause an exponential decrease in accuracy.

\begin{table}[!h]
	\begin{center}
		\begin{tabular}{| c | c |}
			\hline
			\textbf{Data-Set} & \textbf{Accuracy}\\
			\hline
			Var0\_04 & 0.9867\\
			\hline
			Var0\_12 & 0.9823\\
			\hline
			Var0\_20 & 0.9756\\
			\hline
			Var0\_28 & 0.9649\\
			\hline
			Var0\_36 & 0.9478\\
			\hline
			Var0\_44 & 0.9382\\
			\hline
			Var0\_52 & 0.9266\\
			\hline
			Var0\_60 & 0.9093\\
			\hline
		\end{tabular}
		\caption{Blurred MNIST Classification Results: Modified Autoencoder}
		\label{table:modAE_Blurred}
	\end{center}
\end{table}

~\newpage
\noindent The classification result for the blurred and encrypted MNIST data sets, averaged over three tests, is found in Table \ref{table:modAE_BlurredEncrypted}: Blurred and Encrypted MNIST Classification Results: Modified Autoencoder. The highest accuracy of the data sets was 86.56\% for the 0\_04\_en data set, which has the lowest variance of Gaussian noise at 0.04, as outlined in Section \ref{BlurringMethodology}: Blurring. The lowest accuracy of the data sets was 75.04\% for the 0\_60\_en data set, which has the highest variance of Gaussian noise at 0.60. Similar to the results of the original Autoencoder found in Section \ref{EvalOrigAE}, a linear increase in variance is observed to cause an exponential decrease in accuracy in these blurred and encrypted data sets, except for the 0\_12\_en data set which responds as an outlier with a lower than expected average accuracy of 81.52\%. Additional testing may increase this average. Section \ref{AEAnalysis}: Autoencoder Analysis addresses the observation that classification accuracy results for blurred and encrypted data sets is significantly higher in the modified Autoencoder when compared to the original Autoencoder.\\ 

\begin{table}[!h]
	\begin{center}
		\begin{tabular}{| c | c |}
			\hline
			\textbf{Data-Set} & \textbf{Accuracy}\\
			\hline
			0\_04\_en & 0.8656\\
			\hline
			0\_12\_en & 0.8152\\
			\hline
			0\_20\_en & 0.8433\\
			\hline
			0\_28\_en & 0.8311\\
			\hline
			0\_36\_en & 0.8155\\
			\hline
			0\_44\_en & 0.7766\\
			\hline
			0\_52\_en & 0.7864\\
			\hline
			0\_60\_en & 0.7504\\
			\hline
		\end{tabular}
		\caption{Blurred and Encrypted MNIST Classification Results: Modified Autoencoder}
		\label{table:modAE_BlurredEncrypted}
	\end{center}
\end{table}

~\newpage
\noindent A graphical presentation of the classification results for the blurred MNIST data sets compared to the blurred and encrypted MNIST data sets can be observed in Figure \ref{GRAPH_ModifiedAE}: Modified Autoencoder Accuracy Comparison. As stated previously, we can observe that both sets inherit the characteristic that a linear increase in the variance of Gaussian noise results in a slightly exponential decrease in classification accuracy. We can also observe that further encryption has resulted in a marginal decrease in classification accuracy for all blurred data sets. While the encrypted-only MNIST produced a high classification accuracy result of 97.27\%, as stated in Table \ref{table:modAE_Encryption}, the encryption of the blurred-only data sets with such a mild encryption algorithm has marginally decreased their classification accuracy, as found in Table \ref{table:modAE_BlurredEncrypted} and observed in Figure \ref{GRAPH_ModifiedAE} below. The encryption of the blurred data sets has resulted in average in a drop of roughly 15\% in accuracy.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{Mod_AE_BL_vs_BEN}
		\caption{Modified Autoencoder Accuracy Comparison}
		\label{GRAPH_ModifiedAE}
	\end{center}
\end{figure} 

~\newpage
\subsection{Autoencoder Analysis}\label{AEAnalysis}

The classification results of the original and modified Autoencoder for the blurred MNIST data sets can be observed in Figure \ref{GRAPH_COMP_AE_BL}: Autoencoder Accuracy Comparison - Blurred. We observe that the addition of batch normalization has not increased classification accuracy in the modified Autoencoder for most data sets, however a slight divergence is apparent with larger variances of Gaussian noise. The exponential effect of linearly increasing the variance of Gaussian noise is seen in both neural networks.\\

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{Mod_vs_Or_AE_BL}
		\caption{Autoencoder Accuracy Comparison - Blurred}
		\label{GRAPH_COMP_AE_BL}
	\end{center}
\end{figure}

~\newpage
\noindent The classification results of the original and modified Autoencoder for the blurred and encrypted MNIST data sets can be observed in Figure \ref{GRAPH_COMP_AE_BEN}: Autoencoder Accuracy Comparison - Blurred and Encrypted. We observe that the addition of batch normalization has significantly increased classification accuracy in the modified Autoencoder. The data set with the highest accuracy when using the original VGGNet, 0\_04\_en with 46.88\%, has an accuracy of 86.56\% when using the modified Autoencoder. The data set with the lowest converged accuracy when using the original Autoencoder, 0\_36\_en with 17.29\%, has an accuracy of 81.55\% when using the modified Autoencoder. The stabilizing effect of batch normalization on internal covariate shift is clear from these results.\\

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{Mod_vs_Or_AE_BEN}
		\caption{Autoencoder Accuracy Comparison - Blurred and Encrypted}
		\label{GRAPH_COMP_AE_BEN}
	\end{center}
\end{figure}

~\newpage
\section{Conclusions}

\noindent This report presented findings on the classification accuracy of data that has been both blurred and encrypted. We had hypothesized that the classification accuracy of such data in VGGNet and Autoencoder neural networks would be significantly lower than the classification accuracy of data that has been either blurred or encrypted. We further hypothesized that the incorporation of batch normalization into these networks would improve the classification accuracy of the data that had been both blurred and encrypted.\\

\noindent The results of our study has shown that basic VGGNet and Autoencoder neural networks are very accurate in classifying MNIST images that have been blurred by a Gaussian function or encrypted using block encryption techniques. We also found that the basic forms of these neural networks are much less accurate in classifying data that has been both blurred and encrypted, with the VGGNet producing slightly better results than the Autoencoder in such a capacity. It was also noted that a linear increase in variance of Gaussian noise caused an exponential decrease in classification accuracy. The classification results noted here can be observed in Section \ref{EvalOrigVGG} and Section \ref{EvalOrigAE}.\\

\noindent The VGGNet and Autoencoder neural networks produced much better results at classifying data that has been blurred and encrypted after they were modified to include batch normalization. This is due to the stabilizing effect of batch normalization on internal covariate shift. The modified VGGNet neural network had a slightly higher accuracy than the modified Autoencoder in classifying the blurred and encrypted MNIST data. The classification results noted here can be observed in Section \ref{EvalModVGG} and Section \ref{EvalModAE}.\\

\noindent This work addressed an interesting research problem in the field of security, specifically the classification of noisy data. We hope that the finding regarding the benefit of the application of batch normalization into neural networks is helpful in solving problems suffering from internal covariate shift in the future.\\

~\newpage
\section{Acknowledgments}

This work was supported by Dr. He of McMaster University. I would also like to acknowledge Aditya Sharma for the terrific Autoencoder tutorial hosted at the website DataCamp.  


~\newpage

\bibliographystyle {plainnat}
\bibliography {../../References}



\end{document}